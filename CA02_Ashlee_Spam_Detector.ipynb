{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2ada23-c319-4892-8235-ba852da2d2a7",
   "metadata": {},
   "source": [
    "# __CA02:Spam eMail Detection using Naive Bayes Classification Algorithm__\n",
    "2/4/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ebb99-23a9-4bc7-b117-3bf1e3ff12b6",
   "metadata": {},
   "source": [
    "_In this code, we will explore training a model with a set of emails that are either spam or not spam. There are 702 emails total that have are equally divided between spam and not spam. We will test the model on 206 emails and ask the model to compare our known classification with the accuracy of its prediction._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc021b23-390b-4220-a673-409870ec5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashle\\BSAN 6070 Intro to ML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ded5d8-1252-4c6f-a989-a3a8a17f8e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashle\\OneDrive\\Desktop\\BSAN6070\\CA02\n"
     ]
    }
   ],
   "source": [
    "#For my own work, I am seeing where my working directory is to ensure my file path is connected to where the emails are\n",
    "os.chdir(\"C:/Users/ashle/OneDrive/Desktop/BSAN6070/CA02\") \n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3bbd2b-d7e8-4133-95e9-9afae20754f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "Is Directory: True\n"
     ]
    }
   ],
   "source": [
    "#I just want to confirm that my working directory is correct\n",
    "print(\"Exists:\", os.path.exists(\"./train-mails\"))\n",
    "print(\"Is Directory:\", os.path.isdir(\"./train-mails\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbfad04-08da-4924-9e9c-16b455bd982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing neccessary libraries\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#I imported these two in order to proceed with Naive Bayes method\n",
    "from sklearn.naive_bayes import GaussianNB      \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e519571c-e9a1-4515-a2d9-931f640d025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stop words: for this exercise, we are only going to consider the most frequent 3000 words of dictionary from email.\n",
    "\n",
    "nltk.download('stopwords')  # Download stopwords\n",
    "stop_words = set(stopwords.words('english'))  # Here we're choosing the english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673f45d6-c9d0-4064-8762-4ff85c81924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code reads all emails in a directory and extracts and counts word frequencies. In addition, it cleans the data by removing \n",
    "    #non-alphabetic words and single-letter words and returns the 3,000 most frequent words.\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "    all_words = []  # Create an empty list\n",
    "    emails = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]  # Get file names and paths from the emails\n",
    "    \n",
    "    for mail in emails:  # This for loop is going to read each email and split them into individual words and append them to the initialized list.\n",
    "        with open(mail) as m:\n",
    "            for line in m:\n",
    "                words = re.sub(r'[^\\w\\s]', '', line.lower()).split()  # Ensure text is lowercase and punctuation is removed.\n",
    "                all_words += words\n",
    "\n",
    "    dictionary = Counter(all_words)  # Create a Counter dictionary where keys are words, and values are their frequencies\n",
    "    \n",
    "    # Filter out non-alphabetic words and single-letter words using a dictionary comprehension\n",
    "    filtered_dict = {word: count for word, count in dictionary.items() if word.isalpha() and len(word) > 1}\n",
    "    \n",
    "    # Get the 3000 most common words\n",
    "    dictionary = Counter(filtered_dict).most_common(3000) # Get back the 3000 most frequently occurring words\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b33d24b0-181f-4dee-b6ae-c4e64a5e7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mail_dir, dictionary):  # with this function, we are converting emails into a numerical representation based on word frequencies\n",
    "    files = [os.path.join(mail_dir, fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files), 3000))  # Here we're initializing a feature matrix with each row representing an email; each column an email\n",
    "    train_labels = np.zeros(len(files))  # Labels for training (0 for not spam, 1 for spam)\n",
    "    docID = 0\n",
    "    \n",
    "    word_list = [word[0] for word in dictionary]  # Creating the word list from the dictionary (words only)\n",
    "    word_to_index = {word: idx for idx, word in enumerate(word_list)}  # Map words to their index\n",
    "    \n",
    "    for fil in files:  # Looping through each email\n",
    "        with open(fil) as fi:  # Open each email\n",
    "            for i, line in enumerate(fi):\n",
    "                if i == 2: # looking at just the third line (probably because the emails contain 2 lines of headers and we only want to look at the subjects)\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        # Check if the word exists in the dictionary and update the features matrix\n",
    "                        if word in word_to_index:\n",
    "                            wordID = word_to_index[word]\n",
    "                            features_matrix[docID, wordID] += 1  # Increment word count for this document\n",
    "    \n",
    "        # Assign label based on the filename (if it starts with 'spmsg' it's spam)\n",
    "        train_labels[docID] = 1 if fil.lower().startswith(\"spmsg\") else 0\n",
    "        docID += 1\n",
    "    \n",
    "    return features_matrix, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39d32cf5-7f80-4e17-9895-ca24aa377ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As per the \"Important Note\", make sure the the path of your data folders 'train-mails' and 'test-mails' are './train-mails' and './test-mails'.\n",
    "    #This means you must have your .ipynb file and these folders in the SAME FOLDER in your laptop or Google Drive.\n",
    "\n",
    "TRAIN_DIR = './train-mails'\n",
    "TEST_DIR = './test-mails'\n",
    "\n",
    "\n",
    "train_dict = make_Dictionary(TRAIN_DIR) #Here I am creating dictionaries for both training/testing based on the 3000 most frequent words\n",
    "test_dict = make_Dictionary(TEST_DIR)\n",
    "\n",
    "\n",
    "features_matrix, labels = extract_features(TRAIN_DIR, train_dict) #feature extraction from training dataset\n",
    "test_features_matrix, test_labels = extract_features(TEST_DIR, test_dict) #feature extraction from testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbf5a271-2766-4c09-8890-278b217d3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('email', 1664), ('order', 1414), ('address', 1299), ('report', 1217), ('mail', 1133), ('language', 1099), ('send', 1080), ('program', 1009), ('our', 991), ('list', 946), ('one', 921), ('subject', 913), ('name', 883), ('receive', 826), ('free', 801), ('money', 797), ('nt', 759), ('work', 756), ('information', 684), ('business', 669)]\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce0048e8-e3f3-4649-8e32-f82c6c4768fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('university', 582), ('language', 497), ('email', 452), ('http', 397), ('information', 361), ('subject', 350), ('our', 342), ('address', 336), ('com', 331), ('de', 325), ('conference', 314), ('one', 269), ('order', 248), ('please', 248), ('paper', 237), ('program', 232), ('www', 226), ('include', 224), ('web', 223), ('workshop', 223)]\n"
     ]
    }
   ],
   "source": [
    "print(test_dict[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4faf5e1-fb85-46c7-8e55-8a2cfd75dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email: 1664\n",
      "order: 1414\n",
      "address: 1299\n",
      "report: 1217\n",
      "mail: 1133\n",
      "language: 1099\n",
      "send: 1080\n",
      "program: 1009\n",
      "our: 991\n",
      "list: 946\n",
      "one: 921\n",
      "subject: 913\n",
      "name: 883\n",
      "receive: 826\n",
      "free: 801\n",
      "money: 797\n",
      "nt: 759\n",
      "work: 756\n",
      "information: 684\n",
      "business: 669\n"
     ]
    }
   ],
   "source": [
    "top_20_trainwords = train_dict[:20]  # Get the top 20 most common words from the dictionary\n",
    "for word, count in top_20_trainwords:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "babcaa0e-4fbe-4664-ab8d-f7f202cdb11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "university: 582\n",
      "language: 497\n",
      "email: 452\n",
      "http: 397\n",
      "information: 361\n",
      "subject: 350\n",
      "our: 342\n",
      "address: 336\n",
      "com: 331\n",
      "de: 325\n",
      "conference: 314\n",
      "one: 269\n",
      "order: 248\n",
      "please: 248\n",
      "paper: 237\n",
      "program: 232\n",
      "www: 226\n",
      "include: 224\n",
      "web: 223\n",
      "workshop: 223\n"
     ]
    }
   ],
   "source": [
    "top_20_testwords = test_dict[:20]  # Get the top 20 most common words from the dictionary\n",
    "for word, count in top_20_testwords:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "319d41df-cc53-4c14-bc7c-c709ff3ed409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make this code even better, I would further investigate some of these words and why they are appearing\n",
    "#For example... \"www\", \"de\", \"nt\" and\"come\" do not seem to be english words. I want to know why these appear and why they are frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e6064-de2c-4405-b1af-21be4c9f250e",
   "metadata": {},
   "source": [
    "## __Training, Predicting, and Evaluating Model__\n",
    "_Now that we have processed our emails, we can proceed with model traning, predicting, and evaluating_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af0775-998a-424b-8864-bacb28bdb588",
   "metadata": {},
   "source": [
    "### Training our model using Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f09aa824-7265-4933-acbf-00066216b681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n",
      "Testing trained model to predict Test Data labels...\n"
     ]
    }
   ],
   "source": [
    "#model = GaussianNB() #got an accuracy of 1?\n",
    "model = MultinomialNB() #again, got an accuracy of 1?\n",
    "#model = BernoulliNB()\n",
    "\n",
    "model.fit(features_matrix, labels)\n",
    "print(\"Training completed\")\n",
    "print(\"Testing trained model to predict Test Data labels...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee467f-b626-4bf5-b767-01f9fb27dfa3",
   "metadata": {},
   "source": [
    "### Predicting labels for the test data based on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808f9aa0-64b0-49e7-a420-996dd4ccd7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed classification of the Test Data ....\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict(test_features_matrix)\n",
    "print(\"Completed classification of the Test Data ....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc008e5-a1fe-45db-aa11-807074a93a9c",
   "metadata": {},
   "source": [
    "### Evaluating performance (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5a0514a-21bc-4c72-a8a8-8c0c2e29ebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\") \n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9ba3485-702b-440d-b716-a01a18800e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       260\n",
      "\n",
      "    accuracy                           1.00       260\n",
      "   macro avg       1.00      1.00      1.00       260\n",
      "weighted avg       1.00      1.00      1.00       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I keep getting an accuracy of 1.0 regardless of model and attempting to look at overfitting issues...\n",
    "    #I wanted to take a look at other metrics but this seems sketchy hm......\n",
    "\n",
    "print(classification_report(test_labels, predicted_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b5c11192-14a8-4d44-bf63-d08b350d87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n",
      "True: 0.0 Predicted: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"True:\", test_labels[i], \"Predicted:\", predicted_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34b8af-ac39-475e-a40b-68ed9655c328",
   "metadata": {},
   "source": [
    "### My initial issue I ended up finding was that all of the messages were being incorrectly classified as \"not spam\" (labeled as 0) because none of them met the requirement \"if lastToken.startswith(\"spmsg\")\". In looking deeper into what seems to be spam or not, there are many other details that may contribute to this\n",
    "\n",
    "However, because the logic is using simply the file name in this case, it makes sense that they are being classified correctly. It would be a better model if we were looking into the messages too and trying to decipher if the content shows it is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943b6f9-3520-4b94-8143-50f874ceca9f",
   "metadata": {},
   "source": [
    "Desired output: reading and processing emails from TRAIN and TEST folders\r\n",
    "Training Model using Gaussian Naibe Bayes algorithm .....\r\n",
    "Training completed\r\n",
    "testing trained model to predict Test Data labels\r\n",
    "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\r\n",
    "0.9653846153846154"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
